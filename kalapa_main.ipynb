{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('input/train.csv')\n",
    "test = pd.read_csv('input/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Preprocessing </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([train, test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.replace('Tỉnh Hòa Bình', 'Tỉnh Hoà Bình', inplace=True)\n",
    "data.replace('Tỉnh Vĩnh phúc', 'Tỉnh Vĩnh Phúc', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filling missing and nan value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_cols = [\n",
    "    'FIELD_8', 'FIELD_9', 'FIELD_10','FIELD_13','FIELD_17','FIELD_24','FIELD_39', 'FIELD_43',\n",
    "]\n",
    "not_use_cols = ['id', 'label', 'maCv', 'province', 'district', 'FIELD_7']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[['age_source1', 'age_source2']] = data[['age_source1', 'age_source2']].fillna(-999)\n",
    "data[['province', 'district', 'maCv'] + category_cols] = data[['province', 'district', 'maCv'] + category_cols].fillna('Missing')\n",
    "data[['FIELD_50', 'FIELD_51', 'FIELD_52', 'FIELD_53']] = data[['FIELD_50', 'FIELD_51', 'FIELD_52', 'FIELD_53']].fillna(-99.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_map = {'NaN': -1,\n",
    "           'Zero': 0,\n",
    "           'One':1,\n",
    "           'Two':2,\n",
    "           'Three':3,\n",
    "           'Four':4}\n",
    "data['FIELD_35'] = data['FIELD_35'].fillna('NaN').map(dict_map)\n",
    "\n",
    "dict_map = {'NaN': -1,\n",
    "           'I': 1,\n",
    "           'II':2,\n",
    "           'III':3,\n",
    "           'IV':4,\n",
    "           'V':4}\n",
    "data['FIELD_41'] = data['FIELD_41'].fillna('NaN').map(dict_map)\n",
    "\n",
    "dict_map = {'NaN': -1,\n",
    "           'Zezo': 0,\n",
    "           'One':1}\n",
    "data['FIELD_42'] = data['FIELD_42'].fillna('NaN').map(dict_map)\n",
    "\n",
    "dict_map = {'NaN': -1,\n",
    "           'One':1,\n",
    "           'Two':2}\n",
    "data['FIELD_44'] = data['FIELD_44'].fillna('NaN').map(dict_map)\n",
    "\n",
    "dict_map = {'NaN': -999,\n",
    "            'None': -1,\n",
    "            'TRUE': 1,\n",
    "            'FALSE':0\n",
    "}\n",
    "bool_cols = ['FIELD_18','FIELD_19','FIELD_20','FIELD_23','FIELD_25','FIELD_26','FIELD_27','FIELD_28','FIELD_29','FIELD_30',\n",
    "            'FIELD_31','FIELD_36','FIELD_37','FIELD_38']\n",
    "for col in bool_cols:\n",
    "    data[col] = data[col].fillna('NaN').map(dict_map)\n",
    "numberic_cols = ['FIELD_3', 'FIELD_4', 'FIELD_5', 'FIELD_6', 'FIELD_11', 'FIELD_21', 'FIELD_22', 'FIELD_45', 'FIELD_50', 'FIELD_51'\n",
    "                , 'FIELD_52', 'FIELD_53']\n",
    "for col in numberic_cols:\n",
    "    data[col] = data[col].fillna(-999)\n",
    "\n",
    "todo_cols = ['FIELD_54', 'FIELD_55', 'FIELD_56', 'FIELD_57']\n",
    "for col in todo_cols:\n",
    "    data[col] = data[col].fillna(-99.0).replace(['nan'], -1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data[\"maCv2\"] = data[\"maCv\"].str.lower()\n",
    "# def job_category(maCv):\n",
    "#     x = maCv.lower()\n",
    "#     if type(x) == str:\n",
    "#         if \"công nhân\" in x or \"cnv\" in x or \"thợ may\" in x or \"cn\" in x or \"may công nghiệp\" in x \\\n",
    "#         or \"thợ sơn\" in x or \"coõng nhaõn trửùc tieỏp maựy may coõng nghieọp\" in x:\n",
    "#             return \"CN\"\n",
    "#         elif \"giáo viên\" in x:\n",
    "#             return \"GV\"\n",
    "#         elif \"nhân viên\" in x or \"kế toán\" in x or \"cán bộ\" in x or \"nv\" in x:\n",
    "#             return \"NV\"\n",
    "#         elif \"tài xế\" in x or \"lái xe\" in x:\n",
    "#             return \"TX\"\n",
    "#         elif \"undefined\" in x:\n",
    "#             return \"undefined\"\n",
    "#         else:\n",
    "#             return x\n",
    "#     else:\n",
    "#         return x\n",
    "# data['maCv2'] = data['maCv'].apply(job_category).astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Feature Engineering </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['province'] = data['province'].apply(lambda x : str(x).lower())\n",
    "data['district'] = data['district'].apply(lambda x : str(x).lower())\n",
    "data['maCv'] = data['maCv'].apply(lambda x : str(x).lower())\n",
    "data['is_province'] = data['province'].apply(lambda x : str(x).startswith('tỉnh')).astype('bool')\n",
    "data['is_district'] = data['district'].apply(lambda x : str(x).startswith('huyện')).astype('bool')\n",
    "data['len_FIELD_7'] = data['district'].apply(lambda x : len(list(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data['province'].apply(lambda x : str(x).startswith('tỉnh')).astype('bool')\n",
    "data['FIELD_3_by_day'] = (data['FIELD_3'] - 337) % 365\n",
    "# data['FIELD_3_by_month'] = (data['FIELD_3'] - 337) // 12\n",
    "# data['FIELD_3_by_year'] = (data['FIELD_3'] - 337) // 365"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(data['FIELD_4'].unique())\n",
    "# print(data['FIELD_5'].unique())\n",
    "# print(data['FIELD_6'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data['autofe_456'] = data['FIELD_4']*100 + data['FIELD_5']*10 + data['FIELD_6']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data['auto_FIELD_4_5_6'] = data['FIELD_4']*100 + data['']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyvi import ViTokenizer as tokenizer\n",
    "def tokenize(s):\n",
    "    return tokenizer.tokenize(str(s)).lower()\n",
    "train['maCv'] = train['maCv'].apply(lambda x: tokenize(x))\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer().fit(data['maCv'])\n",
    "vec = vectorizer.transform(data['maCv'])\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "svd = TruncatedSVD(n_components=3, n_iter=7, random_state=42).fit(vec)\n",
    "svd_vec = svd.transform(vec)\n",
    "data['maCv_vec0'], data['maCv_vec1'], data['maCv_vec2'] = svd_vec[:, 0], svd_vec[:, 1], svd_vec[: , 2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import ast\n",
    "field_7_str = data['FIELD_7'].fillna('[]').astype(str).apply(lambda x: ' '.join(ast.literal_eval(x)))\n",
    "vectorizer = TfidfVectorizer().fit(field_7_str)\n",
    "vec = vectorizer.transform(field_7_str)\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "svd = TruncatedSVD(n_components=2, n_iter=7, random_state=42).fit(vec)\n",
    "svd_vec = svd.transform(vec)\n",
    "data['FIELD_7_vec0'], data['FIELD_7_vec1'] = svd_vec[:, 0], svd_vec[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in category_cols + ['province', 'district', 'maCv']:\n",
    "    field = dict(data[col].value_counts())\n",
    "#     to_replace = []\n",
    "#     for key, value in field.items():\n",
    "#         if value < 2:\n",
    "#             to_replace.append(key)\n",
    "#     data[col] = data[col].replace(to_replace, 'UNKNOW')\n",
    "    data[col+'_FREQ'] = data[col].map(field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "for col in category_cols + ['province', 'district', 'maCv']:\n",
    "    le = preprocessing.LabelEncoder().fit(data[col])\n",
    "    data[col] = le.transform(data[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "# for col in ['FIELD_8', 'FIELD_10', 'FIELD_17', 'FIELD_24', 'FIELD_35', 'FIELD_41', 'FIELD_43', 'FIELD_44']:\n",
    "#     data = pd.concat([data,pd.get_dummies(data[col], prefix=col,dummy_na=True)],axis=1)\n",
    "# #     print(data[col].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['diff_age'] = abs(data['age_source1'] - data['age_source2'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_cols = [col for col in data.columns if data[col].dtype == 'object']\n",
    "data[object_cols] = data[object_cols].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30000, 85) (20000, 84)\n"
     ]
    }
   ],
   "source": [
    "train = data[data.label.notnull()]\n",
    "test = data[data.label.isnull()]\n",
    "test = test.drop(columns=['label'])\n",
    "print(train.shape, test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30000, 142) (20000, 141)\n"
     ]
    }
   ],
   "source": [
    "train_woe = pd.read_csv('input/train_fe.csv')\n",
    "test_woe = pd.read_csv('input/test_fe.csv')\n",
    "# numberic_cols = ['new_FIELD_3','new_FIELD_50','FIELD_51','FIELD_52','FIELD_53', 'FIELD_54', 'FIELD_55', 'FIELD_56', 'FIELD_57']\n",
    "fe_cols = [col for col in train_woe.columns if col not in ['id', 'label']]\n",
    "train[fe_cols] = train_woe[fe_cols]\n",
    "test[fe_cols] = test_woe[fe_cols]\n",
    "print(train.shape, test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30000, 136) (30000,)\n"
     ]
    }
   ],
   "source": [
    "train_cols = [col for col in train.columns if col not in not_use_cols]\n",
    "train_X = train[train_cols]\n",
    "train_y = train.label\n",
    "print(train_X.shape, train_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    29514\n",
       "1.0      486\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Hyperparams optimize </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini(y_true, y_score):\n",
    "    return roc_auc_score(y_true, y_score)*2 - 1\n",
    "\n",
    "def lgb_gini(y_pred, dataset_true):\n",
    "    y_true = dataset_true.get_label()\n",
    "    return 'gini', gini(y_true, y_pred), True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from bayes_opt import BayesianOptimization\n",
    "\n",
    "# fold__ = 5\n",
    "# def LGB_CV(\n",
    "#           max_depth,\n",
    "#           num_leaves,\n",
    "#           min_data_in_leaf,\n",
    "#           feature_fraction,\n",
    "#           bagging_fraction,\n",
    "#           lambda_l1,\n",
    "#           lambda_l2,\n",
    "#           learning_rate\n",
    "#          ):\n",
    "#     num_round = 10000\n",
    "#     kfold = 5\n",
    "#     folds = StratifiedKFold(n_splits=kfold, shuffle=True, random_state=33)\n",
    "#     oof = np.zeros(len(train_X))\n",
    "\n",
    "#     for fold_, (trn_idx, val_idx) in enumerate(folds.split(train_X.values, train_y.values)):\n",
    "#         print(\"fold n°{}\".format(fold_))\n",
    "#         param = {\n",
    "#             'num_leaves': int(num_leaves),\n",
    "#             'min_data_in_leaf': int(min_data_in_leaf), \n",
    "#             'objective':'binary',\n",
    "#             'max_depth': int(max_depth),\n",
    "#             'learning_rate': learning_rate,\n",
    "#             \"feature_fraction\": feature_fraction,\n",
    "#             \"bagging_freq\": 1,\n",
    "#             \"bagging_fraction\": bagging_fraction ,\n",
    "#             \"bagging_seed\": 11,\n",
    "#             \"metric\": 'auc',\n",
    "#             \"lambda_l1\": lambda_l1,\n",
    "#             \"lambda_l2\": lambda_l2,\n",
    "#             'n_estimators': 1500,\n",
    "#             'boosting' : 'gbdt',\n",
    "#             \"verbosity\": -1,\n",
    "#             'seed':int(2**fold__),\n",
    "#             'bagging_seed':int(2**fold__),\n",
    "#             'drop_seed':int(2**fold__)\n",
    "#         }\n",
    "#         trn_data = lgb.Dataset(train_X.iloc[trn_idx][train_cols], label=train_y.iloc[trn_idx], categorical_feature=category_cols)\n",
    "#         val_data = lgb.Dataset(train_X.iloc[val_idx][train_cols], label=train_y.iloc[val_idx], categorical_feature=category_cols)\n",
    "#         clf = lgb.train(param, trn_data, num_round, feval = lgb_gini, valid_sets = [trn_data, val_data], verbose_eval=100, early_stopping_rounds=500, categorical_feature=category_cols)\n",
    "#         oof[val_idx] = clf.predict(train_X.iloc[val_idx][train_cols], num_iteration=clf.best_iteration)\n",
    "#         score_arr[fold_] = 2*roc_auc_score(train_y.iloc[val_idx], oof[val_idx]) - 1\n",
    "# #         gc.collect()\n",
    "        \n",
    "#     return np.mean(score_arr)\n",
    "# LGB_BO = BayesianOptimization(LGB_CV, {\n",
    "#     'max_depth': (4, 10),\n",
    "#     'num_leaves': (5, 130),\n",
    "#     'min_data_in_leaf': (10, 150),\n",
    "#     'feature_fraction': (0.7, 1.0),\n",
    "#     'bagging_fraction': (0.7, 1.0),\n",
    "#     'lambda_l1': (0, 1),\n",
    "#     'lambda_l2': (0, 6),\n",
    "#     'learning_rate':(0.005, 0.01)\n",
    "# })\n",
    "\n",
    "# LGB_BO.maximize(init_points=2, n_iter=30, acq='ei', xi=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LGB_BO.max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Training model </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0\n",
      "Training until validation scores don't improve for 500 rounds.\n",
      "[100]\ttraining's auc: 0.888015\ttraining's gini: 0.77603\tvalid_1's auc: 0.680773\tvalid_1's gini: 0.361546\n",
      "[200]\ttraining's auc: 0.906726\ttraining's gini: 0.813453\tvalid_1's auc: 0.681775\tvalid_1's gini: 0.363551\n",
      "[300]\ttraining's auc: 0.915783\ttraining's gini: 0.831565\tvalid_1's auc: 0.682348\tvalid_1's gini: 0.364695\n",
      "[400]\ttraining's auc: 0.922261\ttraining's gini: 0.844522\tvalid_1's auc: 0.678605\tvalid_1's gini: 0.35721\n",
      "[500]\ttraining's auc: 0.926748\ttraining's gini: 0.853495\tvalid_1's auc: 0.674456\tvalid_1's gini: 0.348913\n",
      "[600]\ttraining's auc: 0.930922\ttraining's gini: 0.861843\tvalid_1's auc: 0.672479\tvalid_1's gini: 0.344958\n",
      "[700]\ttraining's auc: 0.934613\ttraining's gini: 0.869226\tvalid_1's auc: 0.668474\tvalid_1's gini: 0.336947\n",
      "Early stopping, best iteration is:\n",
      "[214]\ttraining's auc: 0.908039\ttraining's gini: 0.816079\tvalid_1's auc: 0.685466\tvalid_1's gini: 0.370932\n",
      "Fold 1\n",
      "Training until validation scores don't improve for 500 rounds.\n",
      "[100]\ttraining's auc: 0.877536\ttraining's gini: 0.755071\tvalid_1's auc: 0.655679\tvalid_1's gini: 0.311358\n",
      "[200]\ttraining's auc: 0.900143\ttraining's gini: 0.800286\tvalid_1's auc: 0.665271\tvalid_1's gini: 0.330541\n",
      "[300]\ttraining's auc: 0.914208\ttraining's gini: 0.828416\tvalid_1's auc: 0.669501\tvalid_1's gini: 0.339001\n",
      "[400]\ttraining's auc: 0.92156\ttraining's gini: 0.84312\tvalid_1's auc: 0.668764\tvalid_1's gini: 0.337529\n",
      "[500]\ttraining's auc: 0.927236\ttraining's gini: 0.854472\tvalid_1's auc: 0.665799\tvalid_1's gini: 0.331598\n",
      "[600]\ttraining's auc: 0.932015\ttraining's gini: 0.864029\tvalid_1's auc: 0.664997\tvalid_1's gini: 0.329995\n",
      "[700]\ttraining's auc: 0.935348\ttraining's gini: 0.870696\tvalid_1's auc: 0.664117\tvalid_1's gini: 0.328234\n",
      "[800]\ttraining's auc: 0.939448\ttraining's gini: 0.878896\tvalid_1's auc: 0.662502\tvalid_1's gini: 0.325003\n",
      "Early stopping, best iteration is:\n",
      "[310]\ttraining's auc: 0.915447\ttraining's gini: 0.830893\tvalid_1's auc: 0.671558\tvalid_1's gini: 0.343116\n",
      "Fold 2\n",
      "Training until validation scores don't improve for 500 rounds.\n",
      "[100]\ttraining's auc: 0.88782\ttraining's gini: 0.775641\tvalid_1's auc: 0.642583\tvalid_1's gini: 0.285165\n",
      "[200]\ttraining's auc: 0.906331\ttraining's gini: 0.812662\tvalid_1's auc: 0.647581\tvalid_1's gini: 0.295162\n",
      "[300]\ttraining's auc: 0.918698\ttraining's gini: 0.837396\tvalid_1's auc: 0.64186\tvalid_1's gini: 0.283719\n",
      "[400]\ttraining's auc: 0.924776\ttraining's gini: 0.849552\tvalid_1's auc: 0.638262\tvalid_1's gini: 0.276524\n",
      "[500]\ttraining's auc: 0.929276\ttraining's gini: 0.858551\tvalid_1's auc: 0.633906\tvalid_1's gini: 0.267812\n",
      "[600]\ttraining's auc: 0.933398\ttraining's gini: 0.866797\tvalid_1's auc: 0.624849\tvalid_1's gini: 0.249698\n",
      "Early stopping, best iteration is:\n",
      "[180]\ttraining's auc: 0.90258\ttraining's gini: 0.80516\tvalid_1's auc: 0.651778\tvalid_1's gini: 0.303555\n",
      "Fold 3\n",
      "Training until validation scores don't improve for 500 rounds.\n",
      "[100]\ttraining's auc: 0.883453\ttraining's gini: 0.766907\tvalid_1's auc: 0.610438\tvalid_1's gini: 0.220877\n",
      "[200]\ttraining's auc: 0.906166\ttraining's gini: 0.812332\tvalid_1's auc: 0.621768\tvalid_1's gini: 0.243537\n",
      "[300]\ttraining's auc: 0.918835\ttraining's gini: 0.83767\tvalid_1's auc: 0.619191\tvalid_1's gini: 0.238381\n",
      "[400]\ttraining's auc: 0.924572\ttraining's gini: 0.849144\tvalid_1's auc: 0.614088\tvalid_1's gini: 0.228175\n",
      "[500]\ttraining's auc: 0.930739\ttraining's gini: 0.861477\tvalid_1's auc: 0.612352\tvalid_1's gini: 0.224703\n",
      "[600]\ttraining's auc: 0.93454\ttraining's gini: 0.869079\tvalid_1's auc: 0.613888\tvalid_1's gini: 0.227777\n",
      "[700]\ttraining's auc: 0.937639\ttraining's gini: 0.875278\tvalid_1's auc: 0.615396\tvalid_1's gini: 0.230791\n",
      "Early stopping, best iteration is:\n",
      "[233]\ttraining's auc: 0.911855\ttraining's gini: 0.82371\tvalid_1's auc: 0.623227\tvalid_1's gini: 0.246453\n",
      "Fold 4\n",
      "Training until validation scores don't improve for 500 rounds.\n",
      "[100]\ttraining's auc: 0.884395\ttraining's gini: 0.768789\tvalid_1's auc: 0.636738\tvalid_1's gini: 0.273475\n",
      "[200]\ttraining's auc: 0.905049\ttraining's gini: 0.810098\tvalid_1's auc: 0.640697\tvalid_1's gini: 0.281393\n",
      "[300]\ttraining's auc: 0.915654\ttraining's gini: 0.831307\tvalid_1's auc: 0.646578\tvalid_1's gini: 0.293156\n",
      "[400]\ttraining's auc: 0.923209\ttraining's gini: 0.846418\tvalid_1's auc: 0.647104\tvalid_1's gini: 0.294207\n",
      "[500]\ttraining's auc: 0.928495\ttraining's gini: 0.85699\tvalid_1's auc: 0.647986\tvalid_1's gini: 0.295972\n",
      "[600]\ttraining's auc: 0.932344\ttraining's gini: 0.864689\tvalid_1's auc: 0.651023\tvalid_1's gini: 0.302047\n",
      "[700]\ttraining's auc: 0.935633\ttraining's gini: 0.871267\tvalid_1's auc: 0.647542\tvalid_1's gini: 0.295084\n",
      "[800]\ttraining's auc: 0.939407\ttraining's gini: 0.878814\tvalid_1's auc: 0.645834\tvalid_1's gini: 0.291668\n",
      "[900]\ttraining's auc: 0.942458\ttraining's gini: 0.884915\tvalid_1's auc: 0.643455\tvalid_1's gini: 0.28691\n",
      "[1000]\ttraining's auc: 0.945618\ttraining's gini: 0.891236\tvalid_1's auc: 0.644559\tvalid_1's gini: 0.289117\n",
      "[1100]\ttraining's auc: 0.948112\ttraining's gini: 0.896225\tvalid_1's auc: 0.644122\tvalid_1's gini: 0.288244\n",
      "Early stopping, best iteration is:\n",
      "[617]\ttraining's auc: 0.933039\ttraining's gini: 0.866078\tvalid_1's auc: 0.651752\tvalid_1's gini: 0.303504\n",
      "CV mean score: 0.31351 \n",
      "STD: 0.04212 \n"
     ]
    }
   ],
   "source": [
    "num_round = 10000\n",
    "kfold = 5\n",
    "folds = StratifiedKFold(n_splits=kfold, shuffle=True, random_state=33)\n",
    "oof = np.zeros(len(train_X))\n",
    "prediction = np.zeros(len(test))\n",
    "score_arr = np.zeros(kfold)\n",
    "feature_importance_df = pd.DataFrame()\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(train_X.values, train_y.values)):\n",
    "    print(\"Fold {}\".format(fold_))\n",
    "    param = {'num_leaves': int(127.46944213018052),\n",
    "            'min_data_in_leaf': int(142.8216562633925), \n",
    "            'objective':'binary',\n",
    "            'max_depth': int(9.542177995080976),\n",
    "            'learning_rate': 0.006001371601578423,\n",
    "            \"feature_fraction\": 0.7106289550077538,\n",
    "            \"bagging_freq\": 1,\n",
    "            \"bagging_fraction\":  0.7896193631865245 ,\n",
    "            \"metric\": 'auc',\n",
    "            \"lambda_l1\": 0.27253575066409175,\n",
    "            'n_estimators': 1500,\n",
    "            'boosting' : 'gbdt',\n",
    "            \"verbosity\": -1}\n",
    "    trn_data = lgb.Dataset(train_X.iloc[trn_idx][train_cols], label=train_y.iloc[trn_idx])#, categorical_feature=category_cols)\n",
    "    val_data = lgb.Dataset(train_X.iloc[val_idx][train_cols], label=train_y.iloc[val_idx])#, categorical_feature=category_cols)\n",
    "    clf = lgb.train(param, trn_data, num_round, feval = lgb_gini, valid_sets = [trn_data, val_data], verbose_eval=100, early_stopping_rounds=500)#, categorical_feature=category_cols)\n",
    "    oof[val_idx] = clf.predict(train_X.iloc[val_idx][train_cols], num_iteration=clf.best_iteration)\n",
    "#     prediction += clf.predict(test[train_cols], num_iteration=clf.best_iteration)\n",
    "    score_arr[fold_] = 2*roc_auc_score(train_y.iloc[val_idx], oof[val_idx]) - 1\n",
    "    fold_importance_df = pd.DataFrame()\n",
    "    fold_importance_df[\"feature\"] = train_cols\n",
    "    fold_importance_df[\"importance\"] = clf.feature_importance()\n",
    "    fold_importance_df[\"fold\"] = fold_ + 1\n",
    "    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "    prediction += clf.predict(test[train_cols], num_iteration=clf.best_iteration) / folds.n_splits\n",
    "print(\"CV mean score: {:<8.5f}\".format(np.mean(score_arr)))\n",
    "print(\"STD: {:<8.5f}\".format(np.std(score_arr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cols = (feature_importance_df[[\"feature\", \"importance\"]]\n",
    "        .groupby(\"feature\")\n",
    "        .mean()\n",
    "        .sort_values(by=\"importance\", ascending=False)[:1000].index)\n",
    "best_features = feature_importance_df.loc[feature_importance_df.feature.isin(cols)]\n",
    "\n",
    "plt.figure(figsize=(14,26))\n",
    "sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\",ascending=False))\n",
    "plt.title('LightGBM Features (averaged over folds)')\n",
    "plt.tight_layout()\n",
    "plt.savefig('lgbm_importances.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "submission = pd.read_csv('input/sample_submission.csv')\n",
    "submission['label'] = prediction\n",
    "submission.to_csv('input/submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
